{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The loss function is MSE & The optimizer is Adam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "run 100: Style Loss : 0.011750\n",
            "run 200: Style Loss : 0.011750\n",
            "run 300: Style Loss : 0.011750\n",
            "run 400: Style Loss : 0.011750\n",
            "run 500: Style Loss : 0.011750\n",
            "run 600: Style Loss : 0.011750\n",
            "run 700: Style Loss : 0.011750\n",
            "run 800: Style Loss : 0.011750\n",
            "run 900: Style Loss : 0.011750\n",
            "run 1000: Style Loss : 0.011750\n",
            "run 1100: Style Loss : 0.011750\n",
            "run 1200: Style Loss : 0.011750\n",
            "run 1300: Style Loss : 0.011750\n",
            "run 1400: Style Loss : 0.011750\n",
            "run 1500: Style Loss : 0.011750\n",
            "run 1600: Style Loss : 0.011750\n",
            "run 1700: Style Loss : 0.011750\n",
            "run 1800: Style Loss : 0.011750\n",
            "run 1900: Style Loss : 0.011749\n",
            "run 2000: Style Loss : 0.011749\n",
            "run 2100: Style Loss : 0.011749\n",
            "run 2200: Style Loss : 0.011749\n",
            "run 2300: Style Loss : 0.011749\n",
            "run 2400: Style Loss : 0.011749\n",
            "run 2500: Style Loss : 0.011749\n",
            "DONE...\n"
          ]
        }
      ],
      "source": [
        "from __future__ import print_function\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "import torch.optim as optim\n",
        "import numpy as np \n",
        "import librosa\n",
        "import copy\n",
        "import soundfile as sf\n",
        "\n",
        "#define a class named CNNModel,ref:https://github.com/alishdipani/Neural-Style-Transfer-Audio/blob/master/NeuralStyleTransfer.py\n",
        "class CNNModel(nn.Module):\n",
        "    #initialization method of the class, takes self parameter \n",
        "    def __init__(self):\n",
        "        #call the initialization method of the parent class (nn.Module)\n",
        "        super(CNNModel, self).__init__()\n",
        "        #create a 1D convolutional layer with input channels 1025, output channels 4096, kernel size 3, stride 1, padding 1\n",
        "        self.cnn1 = nn.Conv1d(in_channels=1025, out_channels=4096, kernel_size=3, stride=1, padding=1)\n",
        "        \n",
        "    #define the forward method, takes input x\n",
        "    def forward(self, x):\n",
        "        #pass the input x to the convolutional layer self.cnn1\n",
        "        out = self.cnn1(x)\n",
        "        #flatten the output into a one-dimensional vector\n",
        "        out = out.view(out.size(0), -1)\n",
        "        #return the output\n",
        "        return out\n",
        "\n",
        "#define a class named GramMatrix, inheriting from nn.Module class\n",
        "class GramMatrix(nn.Module):\n",
        "    #define the forward method, takes input\n",
        "    def forward(self, input):\n",
        "        #get the shape (dimensions) of the input\n",
        "        a, b, c = input.size()\n",
        "        #reshape the input into a two-dimensional tensor\n",
        "        features = input.view(a * b, c)\n",
        "        #compute the Gram matrix\n",
        "        G = torch.mm(features, features.t())\n",
        "        #return the normalized Gram matrix\n",
        "        return G.div(a * b * c)\n",
        "\n",
        "#define a class named StyleLoss, inheriting from nn.Module class\n",
        "class StyleLoss(nn.Module):\n",
        "    #initialization method of the class, takes target and weight parameters\n",
        "    def __init__(self, target, weight):\n",
        "        #call the initialization method of the parent class (nn.Module)\n",
        "        super(StyleLoss, self).__init__()\n",
        "        #convert the target value to Variable and multiply by weight\n",
        "        self.target = target.detach() * weight\n",
        "        #create an instance of GramMatrix to compute the Gram matrix\n",
        "        self.gram = GramMatrix()\n",
        "        #create a mean squared error loss function\n",
        "        self.criterion = nn.MSELoss()\n",
        "\n",
        "    #define the forward method, takes input input\n",
        "    def forward(self, input):\n",
        "        #compute the Gram matrix of the input\n",
        "        self.G = self.gram(input)\n",
        "        #compute the loss value\n",
        "        self.loss = self.criterion(self.G, self.target)\n",
        "        #return the input, but actually make no change\n",
        "        return input\n",
        "\n",
        "#define a function to read the audio spectrum, takes filename parameter\n",
        "def read_audio_spectrum(filename):\n",
        "    #load the audio file using the librosa library, specifying duration as 30s\n",
        "    x, fs = librosa.load(filename, duration=30)\n",
        "    #compute the Short-Time Fourier Transform (STFT) of the audio\n",
        "    S = librosa.stft(x, n_fft=2048)\n",
        "    #take the logarithm of the magnitude spectrum of STFT\n",
        "    S = np.log1p(np.abs(S))\n",
        "    #return the log magnitude spectrum and sample rate\n",
        "    return S, fs\n",
        "\n",
        "#define a loss function\n",
        "def get_style_model_and_losses(cnn, style_float, style_weight=2500):\n",
        "    #make a copy of the original CNN model to ensure no modification to the original model\n",
        "    cnn = copy.deepcopy(cnn)\n",
        "    #create an empty list to store style losses\n",
        "    style_losses = []\n",
        "    #create a Sequential model to combine convolutional layers and loss functions\n",
        "    model = nn.Sequential()\n",
        "    #create an instance of GramMatrix to compute the Gram matrix\n",
        "    gram = GramMatrix()\n",
        "    #add the convolutional layers of the original model to the new Sequential model. This convolutional layer is named 'conv_1'\n",
        "    model.add_module('conv_1', cnn.cnn1)\n",
        "    #pass the style image through the model to get the feature representation of the input image\n",
        "    target_feature = model(style_float).clone()\n",
        "    #compute the Gram matrix of the target feature\n",
        "    target_feature_gram = gram(target_feature)\n",
        "    #create a StyleLoss instance, passing the Gram matrix of the target feature and the style weight\n",
        "    style_loss = StyleLoss(target_feature_gram, style_weight)\n",
        "    #add the style loss function to the Sequential model for optimization\n",
        "    model.add_module('style_loss_1', style_loss)\n",
        "    #add the style loss to the style loss list\n",
        "    style_losses.append(style_loss)\n",
        "    return model, style_losses\n",
        "\n",
        "#defines an optimization function that converts inputs to trainable parameters and creates an Adam optimizer\n",
        "def get_input_param_optimizer(input_float):\n",
        "    #convert the input parameters to trainable parameters\n",
        "    input_param = nn.Parameter(input_float.data)\n",
        "    #use Adam optimizer to optimize the parameters\n",
        "    optimizer = optim.Adam([input_param], lr=0.1, betas=(0.9, 0.999), eps=1e-08, weight_decay=0)\n",
        "    return input_param, optimizer\n",
        "\n",
        "#defines a style transfer function that optimizes an input image to match the style of a target image using a specified CNN\n",
        "def run_style_transfer(cnn, style_float, input_float, num_steps=2500, style_weight=2500):\n",
        "    #get the model and style losses\n",
        "    model, style_losses = get_style_model_and_losses(cnn, style_float, style_weight)\n",
        "    #get the input parameters and optimizer\n",
        "    input_param, optimizer = get_input_param_optimizer(input_float)\n",
        "    run = [0]\n",
        "\n",
        "    while run[0] <= num_steps:\n",
        "        def closure():\n",
        "            #zero the gradients\n",
        "            input_param.data.clamp_(0, 1)\n",
        "            optimizer.zero_grad()\n",
        "            #forward pass\n",
        "            style_score = model(input_param)\n",
        "            #compute the style loss\n",
        "            style_score = sum([sl.loss for sl in style_losses])\n",
        "            style_score.backward()\n",
        "            run[0] += 1\n",
        "            #output iteration count and style loss\n",
        "            if run[0] % 100 == 0:\n",
        "                print(\"run {}: Style Loss : {:8f}\".format(run[0], style_score.item()))\n",
        "\n",
        "            return style_score\n",
        "\n",
        "        #perform optimization step\n",
        "        optimizer.step(closure)\n",
        "    #clamp the values of input parameters within a reasonable range\n",
        "    input_param.data.clamp_(0, 1)\n",
        "    return input_param.data\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    #specify the file paths for content audio and style audio\n",
        "    content_audio_name = 'generated/jazz.wav'\n",
        "    style_audio_name = 'generated/generated_audio.wav'\n",
        "\n",
        "    #read the spectrum and sample rate of the style audio and content audio\n",
        "    style_audio, style_sr = read_audio_spectrum(style_audio_name)\n",
        "    content_audio, content_sr = read_audio_spectrum(content_audio_name)\n",
        "\n",
        "   #check if the sample rates of content audio and style audio are the same\n",
        "    if content_sr != style_sr:\n",
        "        exit()\n",
        "\n",
        "     #compute the minimum of sample points of style audio and content audio and crop,ref:https://stackoverflow.com/questions/43204441/how-to-split-the-audio-file-in-python\n",
        "    num_samples = min(style_audio.shape[1], content_audio.shape[1])  \n",
        "    style_audio = style_audio[:, :num_samples]\n",
        "    content_audio = content_audio[:, :num_samples]\n",
        "\n",
        "    #add a dimension to the spectrum arrays of style audio and content audio,ref:https://numpy.org/doc/stable/reference/generated/numpy.expand_dims.html\n",
        "    style_audio = np.expand_dims(style_audio, axis=0)\n",
        "    content_audio = np.expand_dims(content_audio, axis=0)\n",
        "\n",
        "    #convert the spectrum arrays of style audio and content audio to PyTorch Variable objects\n",
        "    style_float = Variable(torch.from_numpy(style_audio))\n",
        "    content_float = Variable(torch.from_numpy(content_audio))\n",
        "\n",
        "    #instantiate the CNNModel() class to create a convolutional neural network model cnn\n",
        "    cnn = CNNModel()\n",
        "    #call the run_style_transfer function to run the style transfer algorithm\n",
        "    output = run_style_transfer(cnn, style_float, content_float)\n",
        "    #process the output by removing the batch dimension and then convert to an array\n",
        "    output = output.squeeze(0).numpy()\n",
        "    #create an array of zeros with the same shape as the output\n",
        "    a = np.zeros_like(output)\n",
        "    #Exponentiate the output array, then subtract 1 to get the new array a\n",
        "    a = np.exp(output) - 1\n",
        "\n",
        "    #use an iterative approach by multiplying the exponentiated output array a with random phase p for inverse Short-Time Fourier Transform\n",
        "    p = 2 * np.pi * np.random.random_sample(a.shape) - np.pi\n",
        "    for _ in range(500):\n",
        "        S = a * np.exp(1j*p)\n",
        "        x = librosa.istft(S)\n",
        "        #use librosa.stft function to get the new frequency domain representation of audio data x and update the random phase p for the next iteration\n",
        "        p = np.angle(librosa.stft(x, n_fft=2048))\n",
        "\n",
        "    #output file name\n",
        "    OUTPUT_FILENAME = 'generated/output_audio_1.wav'\n",
        "    #write the generated audio to file, use style_sr as the sampling rate. Assume content and style audio have the same sampling rate\n",
        "    sf.write(OUTPUT_FILENAME, x, style_sr) \n",
        "    print('DONE...')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The loss function is MAE & The optimizer is SGD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "run 100: Style Loss : 0.068326\n",
            "run 200: Style Loss : 0.068326\n",
            "run 300: Style Loss : 0.068326\n",
            "run 400: Style Loss : 0.068326\n",
            "run 500: Style Loss : 0.068326\n",
            "run 600: Style Loss : 0.068326\n",
            "run 700: Style Loss : 0.068326\n",
            "run 800: Style Loss : 0.068326\n",
            "run 900: Style Loss : 0.068326\n",
            "run 1000: Style Loss : 0.068326\n",
            "run 1100: Style Loss : 0.068326\n",
            "run 1200: Style Loss : 0.068326\n",
            "run 1300: Style Loss : 0.068326\n",
            "run 1400: Style Loss : 0.068326\n",
            "run 1500: Style Loss : 0.068326\n",
            "run 1600: Style Loss : 0.068326\n",
            "run 1700: Style Loss : 0.068326\n",
            "run 1800: Style Loss : 0.068326\n",
            "run 1900: Style Loss : 0.068326\n",
            "run 2000: Style Loss : 0.068326\n",
            "run 2100: Style Loss : 0.068326\n",
            "run 2200: Style Loss : 0.068326\n",
            "run 2300: Style Loss : 0.068326\n",
            "run 2400: Style Loss : 0.068326\n",
            "run 2500: Style Loss : 0.068326\n",
            "DONE...\n"
          ]
        }
      ],
      "source": [
        "from __future__ import print_function\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "import torch.optim as optim\n",
        "import numpy as np \n",
        "import librosa\n",
        "import copy\n",
        "import soundfile as sf\n",
        "\n",
        "#define a class named CNNModel,ref:https://github.com/alishdipani/Neural-Style-Transfer-Audio/blob/master/NeuralStyleTransfer.py\n",
        "class CNNModel(nn.Module):\n",
        "    #initialization method\n",
        "    def __init__(self):\n",
        "        #call the initialization method\n",
        "        super(CNNModel, self).__init__()\n",
        "        #create a 1D convolutional layer with input channels 1025, output channels 4096, kernel size 3, stride 1, padding 1\n",
        "        self.cnn1 = nn.Conv1d(in_channels=1025, out_channels=4096, kernel_size=3, stride=1, padding=1)\n",
        "        \n",
        "    #define the forward method, takes input x\n",
        "    def forward(self, x):\n",
        "        #pass the input x to the convolutional layer self.cnn1\n",
        "        out = self.cnn1(x)\n",
        "        #flatten the output into a one-dimensional vector\n",
        "        out = out.view(out.size(0), -1)\n",
        "        #return the output\n",
        "        return out\n",
        "\n",
        "#define a class named GramMatrix, inheriting from nn.Module class\n",
        "class GramMatrix(nn.Module):\n",
        "    #define the forward method, takes input\n",
        "    def forward(self, input):\n",
        "        #get the shape (dimensions) of the input\n",
        "        a, b, c = input.size()\n",
        "        #reshape the input into a two-dimensional tensor\n",
        "        features = input.view(a * b, c)\n",
        "        #compute the Gram matrix\n",
        "        G = torch.mm(features, features.t())\n",
        "        #return the normalized Gram matrix\n",
        "        return G.div(a * b * c)\n",
        "\n",
        "#define a class named StyleLoss, inheriting from nn.Module class\n",
        "class StyleLoss(nn.Module):\n",
        "    def __init__(self, target, weight):\n",
        "        super(StyleLoss, self).__init__()\n",
        "        self.target = target.detach() * weight\n",
        "        self.gram = GramMatrix()\n",
        "        #use Mean Absolute Error (MAE) loss function,ref:https://neptune.ai/blog/pytorch-loss-functions#Mean-Absolute-Error\n",
        "        self.criterion = nn.L1Loss()\n",
        "\n",
        "    #define the forward method, takes input input\n",
        "    def forward(self, input):\n",
        "        #compute the Gram matrix of the input\n",
        "        self.G = self.gram(input)\n",
        "        #compute the loss value\n",
        "        self.loss = self.criterion(self.G, self.target)\n",
        "        #return the input, but actually make no change\n",
        "        return input\n",
        "\n",
        "#define a function to read the audio spectrum, takes filename parameter\n",
        "def read_audio_spectrum(filename):\n",
        "    #load the audio file using the librosa library, specifying duration as 30s\n",
        "    x, fs = librosa.load(filename, duration=30)\n",
        "    #compute the Short-Time Fourier Transform (STFT) of the audio\n",
        "    S = librosa.stft(x, n_fft=2048)\n",
        "    #take the logarithm of the magnitude spectrum of STFT\n",
        "    S = np.log1p(np.abs(S))\n",
        "    #return the log magnitude spectrum and sample rate\n",
        "    return S, fs\n",
        "\n",
        "#define a loss function\n",
        "def get_style_model_and_losses(cnn, style_float, style_weight=2500):\n",
        "    #make a copy of the original CNN model to ensure no modification to the original model\n",
        "    cnn = copy.deepcopy(cnn)\n",
        "    #create an empty list to store style losses\n",
        "    style_losses = []\n",
        "    #create a Sequential model to combine convolutional layers and loss functions\n",
        "    model = nn.Sequential()\n",
        "    #create an instance of GramMatrix to compute the Gram matrix\n",
        "    gram = GramMatrix()\n",
        "    #add the convolutional layers of the original model to the new Sequential model. This convolutional layer is named 'conv_1'\n",
        "    model.add_module('conv_1', cnn.cnn1)\n",
        "    #pass the style image through the model to get the feature representation of the input image\n",
        "    target_feature = model(style_float).clone()\n",
        "    #compute the Gram matrix of the target feature\n",
        "    target_feature_gram = gram(target_feature)\n",
        "    #create a StyleLoss instance, passing the Gram matrix of the target feature and the style weight\n",
        "    style_loss = StyleLoss(target_feature_gram, style_weight)\n",
        "    #add the style loss function to the Sequential model for optimization\n",
        "    model.add_module('style_loss_1', style_loss)\n",
        "    #add the style loss to the style loss list\n",
        "    style_losses.append(style_loss)\n",
        "    return model, style_losses\n",
        "\n",
        "#Defines an optimization function that converts inputs to trainable parameters and creates an Adam optimizer\n",
        "def get_input_param_optimizer(input_float):\n",
        "    #convert the input parameter to trainable parameter\n",
        "    input_param = nn.Parameter(input_float.data)\n",
        "    #use SGD optimizer to optimize the parameters,ref:https://pytorch.org/docs/stable/optim.html\n",
        "    optimizer = optim.SGD([input_param], lr=0.05, momentum=0.9)  \n",
        "    return input_param, optimizer\n",
        "\n",
        "#defines a style transfer function that optimizes an input image to match the style of a target image using a specified CNN\n",
        "def run_style_transfer(cnn, style_float, input_float, num_steps=2500, style_weight=2500):\n",
        "    #get the model and style losses\n",
        "    model, style_losses = get_style_model_and_losses(cnn, style_float, style_weight)\n",
        "    #get the input parameters and optimizer\n",
        "    input_param, optimizer = get_input_param_optimizer(input_float)\n",
        "    run = [0]\n",
        "\n",
        "    while run[0] <= num_steps:\n",
        "        def closure():\n",
        "            #zero the gradients\n",
        "            input_param.data.clamp_(0, 1)\n",
        "            optimizer.zero_grad()\n",
        "            #forward pass\n",
        "            style_score = model(input_param)\n",
        "            #compute the style loss\n",
        "            style_score = sum([sl.loss for sl in style_losses])\n",
        "            style_score.backward()\n",
        "            run[0] += 1\n",
        "            #output iteration count and style loss\n",
        "            if run[0] % 100 == 0:\n",
        "                print(\"run {}: Style Loss : {:8f}\".format(run[0], style_score.item()))\n",
        "\n",
        "            return style_score\n",
        "\n",
        "        #perform optimization step\n",
        "        optimizer.step(closure)\n",
        "    #clamp the values of input parameters within a reasonable range\n",
        "    input_param.data.clamp_(0, 1)\n",
        "    return input_param.data\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    #specify the file paths for content audio and style audio\n",
        "    content_audio_name = 'generated/jazz.wav'\n",
        "    style_audio_name = 'generated/generated_audio.wav'\n",
        "\n",
        "    #read the spectrum and sample rate of the style audio and content audio\n",
        "    style_audio, style_sr = read_audio_spectrum(style_audio_name)\n",
        "    content_audio, content_sr = read_audio_spectrum(content_audio_name)\n",
        "\n",
        "   #check if the sample rates of content audio and style audio are the same\n",
        "    if content_sr != style_sr:\n",
        "        exit()\n",
        "\n",
        "    #compute the minimum of sample points of style audio and content audio and crop,ref:https://stackoverflow.com/questions/43204441/how-to-split-the-audio-file-in-python\n",
        "    num_samples = min(style_audio.shape[1], content_audio.shape[1])  \n",
        "    style_audio = style_audio[:, :num_samples]\n",
        "    content_audio = content_audio[:, :num_samples]\n",
        "\n",
        "    #add a dimension to the spectrum arrays of style audio and content audio,ref:https://numpy.org/doc/stable/reference/generated/numpy.expand_dims.html\n",
        "    style_audio = np.expand_dims(style_audio, axis=0)\n",
        "    content_audio = np.expand_dims(content_audio, axis=0)\n",
        "\n",
        "    #convert the spectrum arrays of style audio and content audio to PyTorch Variable objects\n",
        "    style_float = Variable(torch.from_numpy(style_audio))\n",
        "    content_float = Variable(torch.from_numpy(content_audio))\n",
        "\n",
        "    #instantiate the CNNModel() class to create a convolutional neural network model cnn\n",
        "    cnn = CNNModel()\n",
        "    #call the run_style_transfer function to run the style transfer algorithm\n",
        "    output = run_style_transfer(cnn, style_float, content_float)\n",
        "    #process the output by removing the batch dimension and then convert to an array\n",
        "    output = output.squeeze(0).numpy()\n",
        "    #create an array of zeros with the same shape as the output\n",
        "    a = np.zeros_like(output)\n",
        "    #Exponentiate the output array, then subtract 1 to get the new array a\n",
        "    a = np.exp(output) - 1\n",
        "\n",
        "    #use an iterative approach by multiplying the exponentiated output array a with random phase p for inverse Short-Time Fourier Transform\n",
        "    p = 2 * np.pi * np.random.random_sample(a.shape) - np.pi\n",
        "    for _ in range(500):\n",
        "        S = a * np.exp(1j*p)\n",
        "        x = librosa.istft(S)\n",
        "        #use librosa.stft function to get the new frequency domain representation of audio data x and update the random phase p for the next iteration\n",
        "        p = np.angle(librosa.stft(x, n_fft=2048))\n",
        "\n",
        "    #output file name\n",
        "    OUTPUT_FILENAME = 'generated/output_audio_2.wav'\n",
        "    #write the generated audio to file, use style_sr as the sampling rate. Assume content and style audio have the same sampling rate\n",
        "    sf.write(OUTPUT_FILENAME, x, style_sr) \n",
        "    print('DONE...')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
